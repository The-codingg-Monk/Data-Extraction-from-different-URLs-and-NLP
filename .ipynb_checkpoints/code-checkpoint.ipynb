{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "a5bf6462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import requests\n",
    "from string import punctuation\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import re\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "991d47be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<head><title>Not Acceptable!</title></head><body><h1>Not Acceptable!</h1><p>An appropriate representation of the requested resource could not be found on this server. This error was generated by Mod_Security.</p></body></html>'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url='https://insights.blackcoffer.com/in-future-or-in-upcoming-years-humans-and-machines-are-going-to-work-together-in-every-field-of-work/'\n",
    "\n",
    "data=requests.get(url)\n",
    "data.text  #Access has been prohibited so instead of beatuful soup we will use scrapy.\n",
    "\n",
    "#Web scrapping using scrapy has been done on vs code, have shared code in vs code folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "39d1882b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“If anything kills over 10 million people in t...</td>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where is this disruptive technology taking us?...</td>\n",
       "      <td>Will machine replace the human in the future o...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Anything that could give rise to smarter-than...</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Machine intelligence is the last invention th...</td>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  “If anything kills over 10 million people in t...   \n",
       "1  Where is this disruptive technology taking us?...   \n",
       "2  Human minds, a fascination in itself carrying ...   \n",
       "3  “Anything that could give rise to smarter-than...   \n",
       "4  “Machine intelligence is the last invention th...   \n",
       "\n",
       "                                               title  url_id  \n",
       "0       AI in healthcare to Improve Patient Outcomes      37  \n",
       "1  Will machine replace the human in the future o...      42  \n",
       "2   What if the Creation is Taking Over the Creator?      38  \n",
       "3  Will Machine Replace The Human in the Future o...      40  \n",
       "4                Will AI Replace Us or Work With Us?      41  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_df=pd.read_csv(r'C:\\Users\\kannu\\OneDrive\\Desktop\\DataScience\\Projects\\Blackcoffer\\20211030 Test Assignment\\vscode\\scraping_text\\scraping_text\\Extracted_data.csv')\n",
    "extracted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "897b57f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>url_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“If anything kills over 10 million people in t...</td>\n",
       "      <td>AI in healthcare to Improve Patient Outcomes</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where is this disruptive technology taking us?...</td>\n",
       "      <td>Will machine replace the human in the future o...</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Human minds, a fascination in itself carrying ...</td>\n",
       "      <td>What if the Creation is Taking Over the Creator?</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>“Anything that could give rise to smarter-than...</td>\n",
       "      <td>Will Machine Replace The Human in the Future o...</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>“Machine intelligence is the last invention th...</td>\n",
       "      <td>Will AI Replace Us or Work With Us?</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  “If anything kills over 10 million people in t...   \n",
       "1  Where is this disruptive technology taking us?...   \n",
       "2  Human minds, a fascination in itself carrying ...   \n",
       "3  “Anything that could give rise to smarter-than...   \n",
       "4  “Machine intelligence is the last invention th...   \n",
       "\n",
       "                                               title  url_id  \n",
       "0       AI in healthcare to Improve Patient Outcomes      37  \n",
       "1  Will machine replace the human in the future o...      42  \n",
       "2   What if the Creation is Taking Over the Creator?      38  \n",
       "3  Will Machine Replace The Human in the Future o...      40  \n",
       "4                Will AI Replace Us or Work With Us?      41  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removind special characters from text data,so that can be stored in txt file.\n",
    "\n",
    "\n",
    "def rm_char(data):\n",
    "    \n",
    "    res=re.sub(\"\\u20b9\",\"\",data)\n",
    "    res=re.sub(\"\\)\",\" \",res)\n",
    "    res=re.sub(\"\\(\",\" \",res)\n",
    "    \n",
    "    return res\n",
    "\n",
    "extracted_df[\"text\"]= extracted_df[\"text\"].apply(rm_char)\n",
    "extracted_df.head()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "93384686",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\"Gathered_texts\")\n",
    "os.mkdir('Gathered_texts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537c4c79",
   "metadata": {},
   "source": [
    "## Saving Extracted data in .txt file having name as url_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "628131d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text,title,url_id in extracted_df.values:\n",
    "    loc=str(url_id)+\".txt\"\n",
    "    with open(os.path.join('Gathered_texts',loc),'a') as file:\n",
    "        file.write(title)\n",
    "        file.write('\\n')\n",
    "        file.write(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ff3dd",
   "metadata": {},
   "source": [
    "## Putting all stopwords in a single place, inside a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3e15de94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "stop_word_paths=glob.glob('StopWords\\*.txt')     #getting stopwords paths\n",
    "for path in stop_word_paths:\n",
    "    with open(path,'r') as file:\n",
    "        data=file.read()\n",
    "        with open('stop_words.txt','a') as file1:\n",
    "            file1.write(data)\n",
    "            \n",
    "with open('stop_words.txt','r') as file:\n",
    "    all_stopwords=file.read()\n",
    "\n",
    "cleaned=re.sub(\"\\W+\",\" \",all_stopwords).lower()\n",
    "stopwords_list=[word.lower() for word in re.sub(\"\\d+\",\"\",cleaned).split() if word not in punctuation]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de9769",
   "metadata": {},
   "source": [
    "## Preprocessing negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "88e57024",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"MasterDictionary/negative-words.txt\") as file:\n",
    "    negative_data=file.read()\n",
    "    \n",
    "cleaned=re.sub(\"\\W+\",\" \",negative_data)\n",
    "cleaned_neg_text=[word.lower() for word in word_tokenize(re.sub(\"\\d+\",\"\",cleaned)) if (word not in punctuation) and (word.lower() not in stopwords_list)]\n",
    "\n",
    "#Performing lemmatization to convert each word into its root word.\n",
    "final_neg_words=[]\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "for word in cleaned_neg_text:\n",
    "    final_neg_words.append(lemma.lemmatize(word,'v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576011cd",
   "metadata": {},
   "source": [
    "## Preprocessing positive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "813c8d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"MasterDictionary/positive-words.txt\") as file:\n",
    "    positive_data=file.read()\n",
    "    \n",
    "cleaned=re.sub(\"\\W+\",\" \",positive_data)\n",
    "cleaned_pos_text=[word.lower() for word in word_tokenize (re.sub(\"\\d+\",\"\",cleaned)) if (word not in punctuation) and (word.lower() not in stopwords_list)]\n",
    "\n",
    "#Performing lemmatization to convert each word into its root word.\n",
    "final_pos_words=[]\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "for word in cleaned_pos_text:\n",
    "    final_pos_words.append(lemma.lemmatize(word,'v'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccf5559",
   "metadata": {},
   "source": [
    "## Analysing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "0b42d532",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "missing ), unterminated subpattern at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Input \u001b[1;32mIn [149]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m output_text\u001b[38;5;241m.\u001b[39msplit():\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[0;32m     77\u001b[0m             personal_pronouns \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     81\u001b[0m url_no\u001b[38;5;241m.\u001b[39mappend(url_id)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:240\u001b[0m, in \u001b[0;36mfindall\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfindall\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;124;03m\"\"\"Return a list of all non-overlapping matches in the string.\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \n\u001b[0;32m    235\u001b[0m \u001b[38;5;124;03m    If one or more capturing groups are present in the pattern, return\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m \n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m    Empty matches are included in the result.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfindall(string)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\re.py:303\u001b[0m, in \u001b[0;36m_compile\u001b[1;34m(pattern, flags)\u001b[0m\n\u001b[0;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sre_compile\u001b[38;5;241m.\u001b[39misstring(pattern):\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfirst argument must be string or compiled pattern\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 303\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msre_compile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (flags \u001b[38;5;241m&\u001b[39m DEBUG):\n\u001b[0;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(_cache) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _MAXCACHE:\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;66;03m# Drop the oldest item\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_compile.py:764\u001b[0m, in \u001b[0;36mcompile\u001b[1;34m(p, flags)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isstring(p):\n\u001b[0;32m    763\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m p\n\u001b[1;32m--> 764\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43msre_parse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    766\u001b[0m     pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:950\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(str, flags, state)\u001b[0m\n\u001b[0;32m    947\u001b[0m state\u001b[38;5;241m.\u001b[39mstr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\n\u001b[0;32m    949\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 950\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[43m_parse_sub\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m&\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mSRE_FLAG_VERBOSE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Verbose:\n\u001b[0;32m    952\u001b[0m     \u001b[38;5;66;03m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[39;00m\n\u001b[0;32m    953\u001b[0m     \u001b[38;5;66;03m# on the safe side, we'll parse the whole thing again...\u001b[39;00m\n\u001b[0;32m    954\u001b[0m     state \u001b[38;5;241m=\u001b[39m State()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:443\u001b[0m, in \u001b[0;36m_parse_sub\u001b[1;34m(source, state, verbose, nested)\u001b[0m\n\u001b[0;32m    441\u001b[0m start \u001b[38;5;241m=\u001b[39m source\u001b[38;5;241m.\u001b[39mtell()\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 443\u001b[0m     itemsappend(\u001b[43m_parse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnested\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sourcematch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    446\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\sre_parse.py:838\u001b[0m, in \u001b[0;36m_parse\u001b[1;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[0;32m    836\u001b[0m p \u001b[38;5;241m=\u001b[39m _parse_sub(source, state, sub_verbose, nested \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m source\u001b[38;5;241m.\u001b[39mmatch(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 838\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m source\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing ), unterminated subpattern\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    839\u001b[0m                        source\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    841\u001b[0m     state\u001b[38;5;241m.\u001b[39mclosegroup(group, p)\n",
      "\u001b[1;31merror\u001b[0m: missing ), unterminated subpattern at position 0"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "url_no=[]\n",
    "url=[]\n",
    "pos_score=[]\n",
    "neg_score=[]\n",
    "pol_score=[]\n",
    "sub_score=[]\n",
    "avg_sent_length=[]\n",
    "p_of_complex=[]\n",
    "fog_index=[]\n",
    "avg_word_per_sent=[]\n",
    "word_counts=[]\n",
    "syllable_per_word=[]\n",
    "personal_pronounce=[]\n",
    "\n",
    "# getting all paths of .txt files\n",
    "\n",
    "paths=glob.glob(r\"Gathered_texts\\*.txt\")\n",
    "\n",
    "for path in paths:\n",
    "\n",
    "    with open(path,\"r\") as file:\n",
    "        output_text=file.read()\n",
    "        \n",
    "    \n",
    "    cleaned=re.sub(\"\\W+\",\" \",output_text)\n",
    "    word_list=[word.lower() for word in word_tokenize (re.sub(\"\\d+\",\"\",cleaned)) if (word not in punctuation) and (word.lower() not in stopwords_list)]\n",
    "    \n",
    "    final_lemmatized_words=[]\n",
    "    lemma=WordNetLemmatizer()\n",
    "\n",
    "    for word in word_list:\n",
    "        final_lemmatized_words.append(lemma.lemmatize(word,'v'))\n",
    "        \n",
    "    pos=0\n",
    "    neg=0\n",
    "     \n",
    "    for words in  final_lemmatized_words:\n",
    "        if words in final_pos_words:\n",
    "            pos+=1\n",
    "        elif words in final_neg_words:\n",
    "            neg-=1\n",
    "            \n",
    "    neg*=-1\n",
    "    \n",
    "    polarity=(pos-neg)/((pos+neg)+0.000001)\n",
    "    sub=(pos + neg)/ ((len(final_lemmatized_words)) + 0.000001)\n",
    "    no_of_sent=sent_tokenize(output_text)\n",
    "    avg_len=len(final_lemmatized_words)/len(no_of_sent)\n",
    "    \n",
    "    #Finding complex words from sentence\n",
    "    \n",
    "    complex_words=[]\n",
    "    \n",
    "    for word in final_lemmatized_words:\n",
    "        count=0\n",
    "        for char in word:\n",
    "            if char.lower() in [\"a\",\"e\",\"i\",\"o\",\"u\"]:\n",
    "                count+=1\n",
    "                \n",
    "        if count>2 and not(word.endswith(\"es\"))and not(word.endswith(\"ed\")):\n",
    "            complex_words.append(word)\n",
    "            \n",
    "        \n",
    "    per_compl_score=len(complex_words)/len(final_lemmatized_words)\n",
    "    fog=(0.4*avg_len)+per_compl_score\n",
    "    \n",
    "    avg_no_of_words_per_sent=per_compl_score\n",
    "    syllable_count_per_word=complex_words_count=len(complex_words)\n",
    "    word_count=len(final_lemmatized_words)\n",
    "    personal_pronouns=0\n",
    "    \n",
    "    text='i we my ours us'\n",
    "    for word in output_text.split():\n",
    "        \n",
    "        if word !=\"US\":\n",
    "            if bool(re.findall(word.lower(),text)):\n",
    "                personal_pronouns +=1\n",
    "                \n",
    "                \n",
    "    \n",
    "    url_no.append(url_id)\n",
    "    url.append(link)\n",
    "    pos_score.append(pos)\n",
    "    neg_score.append(neg)\n",
    "    pol_score.append(polarity)\n",
    "    sub_score.append(sub)\n",
    "    avg_sent_length.append(avg_len)\n",
    "    p_of_complex.append(per_compl_score)\n",
    "    fog_index.append(fog)\n",
    "    avg_word_per_sent.append(avg_no_of_words_per_sent)\n",
    "    word_counts.append(word_count)\n",
    "    syllable_per_word.append(syllable_count_per_word)\n",
    "    personal_pronounce.append(personal_pronouns)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ed0737",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame({\"URL_ID\":url_no,\"URL\":url,\"POSITIVE SCORE\":pos_score,\"NEGATIVE SCORE\":neg_score,\"POLARITY SCORE\":pol_score,\"SUBJECTIVITY SCORE\":sub_score,\"AVG SENTENCE LENGTH\":avg_sent_length,\"PERCENTAGE OF COMPLEX WORDS\":p_of_complex,\"FOG INDEX\":fog_index,\"AVG NUMBER OF WORDS PER SENTENCE\":avg_word_per_sent,\"WORD COUNT\":word_counts,\"SYLLABLE PER WORD\":syllable_per_word,\"PERSONAL PRONOUNS\":personal_pronounce})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7dafbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65be872",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned=re.sub(\"\\W+\",\" \",output_text)\n",
    "word_list=[word.lower() for word in word_tokenize (re.sub(\"\\d+\",\"\",cleaned)) if (word not in punctuation) and (word.lower() not in stopwords_list)]\n",
    "print(f\"{word_list=}\")\n",
    "final_lemmatized_words=[]\n",
    "lemma=WordNetLemmatizer()\n",
    "\n",
    "for word in word_list:\n",
    "    final_lemmatized_words.append(lemma.lemmatize(word,'v'))\n",
    "\n",
    "pos=0\n",
    "neg=0\n",
    "\n",
    "print(f\"{final_lemmatized_words=}\")\n",
    "\n",
    "for words in  final_lemmatized_words:\n",
    "    if words in final_pos_words:\n",
    "        pos+=1\n",
    "    elif words in final_neg_words:\n",
    "        neg-=1\n",
    "\n",
    "neg*=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739f5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in final_lemmatized_words:\n",
    "    if word in final_pos_words:\n",
    "        print('POS')\n",
    "    elif word in final_neg_words:\n",
    "        print(\"NEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28861f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37cc1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4283d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93f91d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e681db08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d07dfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
